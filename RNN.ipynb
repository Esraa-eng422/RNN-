{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "sentence = [\"i\", \"love\", \"deep\", \"learning\"]\n",
        "vocab = list(set(sentence))\n",
        "\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "hidden_size = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "Wxh = [[0.1, 0.2, 0.1, 0.0], [0.0, 0.1, 0.2, 0.1]]\n",
        "Whh = [[0.1, 0.2], [0.1, 0.1]]\n",
        "Why = [[0.1, 0.1], [0.2, 0.1], [0.1, 0.1], [0.2, 0.2]]\n",
        "bh = [0.0, 0.0]\n",
        "by = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "def one_hot(index, size):\n",
        "    vec = [0] * size\n",
        "    vec[index] = 1\n",
        "    return vec\n",
        "\n",
        "def tanh(x):\n",
        "    return (2 / (1 + math.exp(-2 * x))) - 1\n",
        "\n",
        "def dtanh(y):\n",
        "    return 1 - y ** 2\n",
        "\n",
        "def softmax(logits):\n",
        "    exps = [math.exp(l) for l in logits]\n",
        "    s = sum(exps)\n",
        "    return [e / s for e in exps]\n",
        "\n",
        "def train(inputs, target_index):\n",
        "    x_s = []\n",
        "    h_s = { -1: [0.0] * hidden_size }\n",
        "\n",
        "\n",
        "    for t in range(len(inputs)):\n",
        "        x = one_hot(inputs[t], vocab_size)\n",
        "        x_s.append(x)\n",
        "\n",
        "        h = []\n",
        "        for i in range(hidden_size):\n",
        "            z = sum(Wxh[i][j] * x[j] for j in range(vocab_size)) + \\\n",
        "                sum(Whh[i][j] * h_s[t-1][j] for j in range(hidden_size)) + bh[i]\n",
        "            h.append(tanh(z))\n",
        "        h_s[t] = h\n",
        "\n",
        "\n",
        "    y = []\n",
        "    for i in range(vocab_size):\n",
        "        z = sum(Why[i][j] * h_s[len(inputs)-1][j] for j in range(hidden_size)) + by[i]\n",
        "        y.append(z)\n",
        "    probs = softmax(y)\n",
        "\n",
        "\n",
        "    loss = -math.log(probs[target_index])\n",
        "\n",
        "\n",
        "    dWhy = [[0 for _ in range(hidden_size)] for _ in range(vocab_size)]\n",
        "    dby = [0] * vocab_size\n",
        "    dy = probs[:]\n",
        "    dy[target_index] -= 1\n",
        "\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(hidden_size):\n",
        "            dWhy[i][j] = dy[i] * h_s[len(inputs)-1][j]\n",
        "        dby[i] = dy[i]\n",
        "\n",
        "\n",
        "    dh = [sum(dy[o] * Why[o][h] for o in range(vocab_size)) for h in range(hidden_size)]\n",
        "    dWxh = [[0 for _ in range(vocab_size)] for _ in range(hidden_size)]\n",
        "    dWhh = [[0 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
        "    dbh = [0 for _ in range(hidden_size)]\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dh_raw = [dh[i] * dtanh(h_s[t][i]) for i in range(hidden_size)]\n",
        "        for i in range(hidden_size):\n",
        "            for j in range(vocab_size):\n",
        "                dWxh[i][j] += dh_raw[i] * x_s[t][j]\n",
        "            for j in range(hidden_size):\n",
        "                dWhh[i][j] += dh_raw[i] * h_s[t-1][j]\n",
        "            dbh[i] += dh_raw[i]\n",
        "\n",
        "\n",
        "    def clip(g, v=5):\n",
        "        return max(-v, min(g, v))\n",
        "\n",
        "\n",
        "    for i in range(hidden_size):\n",
        "        for j in range(vocab_size):\n",
        "            Wxh[i][j] -= learning_rate * clip(dWxh[i][j])\n",
        "        for j in range(hidden_size):\n",
        "            Whh[i][j] -= learning_rate * clip(dWhh[i][j])\n",
        "        bh[i] -= learning_rate * clip(dbh[i])\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(hidden_size):\n",
        "            Why[i][j] -= learning_rate * clip(dWhy[i][j])\n",
        "        by[i] -= learning_rate * clip(dby[i])\n",
        "\n",
        "    return loss\n",
        "\n",
        "inputs = [word_to_index[\"i\"], word_to_index[\"love\"], word_to_index[\"deep\"]]\n",
        "target = word_to_index[\"learning\"]\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss = train(inputs, target)\n",
        "    if epoch % 20 == 0:\n",
        "        print(\"Epoch\", epoch, \"Loss:\", round(loss, 4))\n",
        "\n",
        "def predict(input_words):\n",
        "    input_indices = [word_to_index[word] for word in input_words]\n",
        "\n",
        "    h_prev = [0.0 for _ in range(hidden_size)]\n",
        "    for t in range(len(input_indices)):\n",
        "        x = one_hot(input_indices[t], vocab_size)\n",
        "        h = []\n",
        "        for i in range(hidden_size):\n",
        "            z = sum(Wxh[i][j] * x[j] for j in range(vocab_size)) + \\\n",
        "                sum(Whh[i][j] * h_prev[j] for j in range(hidden_size)) + bh[i]\n",
        "            h.append(tanh(z))\n",
        "        h_prev = h\n",
        "\n",
        "    y = []\n",
        "    for i in range(vocab_size):\n",
        "        z = sum(Why[i][j] * h_prev[j] for j in range(hidden_size)) + by[i]\n",
        "        y.append(z)\n",
        "    probs = softmax(y)\n",
        "    predicted_index = probs.index(max(probs))\n",
        "    return index_to_word[predicted_index]\n",
        "\n",
        "test_input = [\"i\", \"love\", \"deep\"]\n",
        "print(\"\\nPredicted word:\", predict(test_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7GXbqz7tnZs",
        "outputId": "a905aaf5-7084-41bf-e5ce-0e4cd0758a29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 1.3943\n",
            "Epoch 20 Loss: 0.4946\n",
            "Epoch 40 Loss: 0.21\n",
            "Epoch 60 Loss: 0.1088\n",
            "Epoch 80 Loss: 0.0692\n",
            "Epoch 100 Loss: 0.0492\n",
            "Epoch 120 Loss: 0.0375\n",
            "Epoch 140 Loss: 0.03\n",
            "Epoch 160 Loss: 0.0248\n",
            "Epoch 180 Loss: 0.021\n",
            "\n",
            "Predicted word: learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkEmvzQVuC5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}